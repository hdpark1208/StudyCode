{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "NLP_CharRNN_practice.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hdpark1208/StudyCode/blob/main/NLP/NLP_CharRNN_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCe8JJRC0NXH"
      },
      "source": [
        "# Char RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8qaZ9u90NXJ"
      },
      "source": [
        "문자 단위 RNN : RNN의 입출력 단위가 단어 레벨(word-level)이 아닌 문자 레벨(Char-level)로 RNN을 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MCmJWlR0NXJ"
      },
      "source": [
        "## 문자 단위 RNN을 다대다 구조로 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBW-kTYM0NXK"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isX18iCA0NXK"
      },
      "source": [
        "### 문자 시퀀스 apple을 입력받으면 pple!를 출력하는 RNN 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAljWW5H0NXK"
      },
      "source": [
        "입력 데이터와 레이블 데이터에 대해서 문자 집합 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NARU_7eW0NXL",
        "outputId": "b7b7de7e-8e8d-42b4-fba2-1b29541ce68b"
      },
      "source": [
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str+label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "print(char_vocab)\n",
        "print('문자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['!', 'a', 'e', 'l', 'p']\n",
            "문자 집합의 크기 : 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PhpLUGz0NXM"
      },
      "source": [
        "input_size = vocab_size # One-hot 사용할 것이므로 입력의 크기는 문자 집합의 크기\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAI6Twko0NXM",
        "outputId": "4f8af69e-67fd-4bb7-cbd7-9f4a0bfd7358"
      },
      "source": [
        "test_list = [1,2,3,4,5]\n",
        "test_value_list=['a','b','c','d','e']\n",
        "test_dict = dict((c,i) for i,c in enumerate(test_value_list))\n",
        "test_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms9L4J890NXM",
        "outputId": "e6584c09-8c30-4ece-b46c-3c729fe56b7d"
      },
      "source": [
        "# 문자 집합에 고유 인덱스 부여\n",
        "char_to_index = dict((c,i) for i,c in enumerate(char_vocab))\n",
        "char_to_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brvPo_A60NXN"
      },
      "source": [
        "나중에 예측 결과를 다시 문자 시퀀스로 보기위해, 정수로부터 문자를 얻을 수 있는 dict 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG-EnXjI0NXN",
        "outputId": "6c426201-5066-47e7-a4d2-e6f1a16b2c11"
      },
      "source": [
        "index_to_char={}\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key\n",
        "index_to_char"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJnih0Uo0NXO"
      },
      "source": [
        "입력 데이터와 레이블 데이터의 각 문자들을 정수로 맵핑"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YorKCt5P0NXO",
        "outputId": "89ae1fc0-979d-455e-caf7-996438b3902c"
      },
      "source": [
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-5E7UGx0NXO",
        "outputId": "f9f45889-7d21-4f1d-a0b6-ebc3c2db8bd4"
      },
      "source": [
        "np.eye(3)[1] # np.eye(n)[m] One-hot 벡터 생성 효과"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyaKdcuz0NXO"
      },
      "source": [
        "* 파이토치의 nn.RNN()은 기본적으로 3차원 텐서를 입력받으므로 배치를 위한 차원 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIPXX0Z-0NXP"
      },
      "source": [
        "x_data = [x_data]\n",
        "y_data = [y_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgF9haZd0NXP",
        "outputId": "077b5c6c-e167-4b40-91f8-4af9239292b4"
      },
      "source": [
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMy5zKs50NXP"
      },
      "source": [
        "데이터를 텐서로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyZu8PBb0NXP"
      },
      "source": [
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGp20i170NXP"
      },
      "source": [
        "텐서의 크기 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPKBwcuQ0NXQ",
        "outputId": "dff3fdb0-92d4-4933-d257-48539077ec14"
      },
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape)) \n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
            "레이블의 크기 : torch.Size([1, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXwQk3A50NXQ"
      },
      "source": [
        "훈련 데이터의 크기 : torch.Size([1, 5, 5])  \n",
        "레이블의 크기 : torch.Size([1, 5])  \n",
        "\n",
        "교재에서의 결과 (배치 사이즈가 표시되어있음)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJSXLas-0NXQ"
      },
      "source": [
        "### 모델 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G4_014o0NXQ"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,output_size):\n",
        "        super(Net,self).__init__()\n",
        "        # RNN 셀 구현\n",
        "        self.rnn = torch.nn.RNN(input_size,hidden_size,batch_first=True)\n",
        "        # 출력층 구현\n",
        "        self.fc = torch.nn.Linear(hidden_size,output_size,bias=True)\n",
        "        \n",
        "    def forward(self,x): # 구현한 RNN 셀과 출력층을 연결\n",
        "        x,_status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJqQzAF90NXQ"
      },
      "source": [
        "모델 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRwGtOEW0NXR"
      },
      "source": [
        "net = Net(input_size,hidden_size,output_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDq6aTAV0NXR"
      },
      "source": [
        "모델에 텐서 입력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-wf39Si0NXR",
        "outputId": "f1fbe26e-9ca1-47c5-9e09-aad3220c84c5"
      },
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 배치 차원, 시점(timesteps), 출력의 크기"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LD8lyKy0NXR"
      },
      "source": [
        "나중에 정확도를 측정할 때는 이를 모두 펼쳐서 계산하게 되는데, 이때는 view를 사용하여 배치 차원과 시점 차원을 하나로 만듭니다 (즉 차원을 하나 낮춘다)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24B-uI2w0NXR",
        "outputId": "55897383-4464-4995-a4f9-dc7c2c9e5b19"
      },
      "source": [
        "print(outputs.view(-1, input_size).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpdcVIF90NXS",
        "outputId": "2fff0e04-ea96-431f-e6e3-6568fcaf794c"
      },
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q5NgpMq0NXS"
      },
      "source": [
        "옵티마이저, 손실 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkeIUiCo0NXS"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdZTYjMd0NXS",
        "outputId": "798b1f95-23dd-404c-865b-0a89c165cf94"
      },
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
        "    loss.backward() # 기울기 계산\n",
        "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
        "\n",
        "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
        "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 loss:  1.6259359121322632 prediction:  [[3 3 3 3 3]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  lllll\n",
            "1 loss:  1.3896162509918213 prediction:  [[4 0 3 3 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  p!ll!\n",
            "2 loss:  1.1391284465789795 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "3 loss:  0.8826513290405273 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "4 loss:  0.6482082605361938 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "5 loss:  0.4503559470176697 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "6 loss:  0.31385722756385803 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "7 loss:  0.21607990562915802 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "8 loss:  0.1435898393392563 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss:  0.0956205278635025 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.06557932496070862 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.046368274837732315 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.0336359441280365 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.024936463683843613 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.018848108127713203 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.01450914703309536 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.011375370435416698 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.009087580256164074 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.0074002547189593315 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.006141324993222952 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.005189078859984875 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.004456750582903624 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.003882983233779669 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.003424228634685278 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.0030498988926410675 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.0027387551963329315 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.0024761680979281664 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.0022518797777593136 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.0020588510669767857 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.0018917706329375505 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.0017468195874243975 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.0016207944136112928 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.001511107082478702 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.0014155254466459155 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.0013321249280124903 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.0012591227423399687 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.0011952115455642343 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.0011390119325369596 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.0010894775623455644 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.0010457279859110713 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.0010068115079775453 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.0009722519898787141 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.0009413356892764568 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.0009135626023635268 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.0008885757997632027 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.0008660418097861111 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.000845603528432548 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.0008269991958513856 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.0008099667611531913 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.0007943871314637363 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.0007800461025908589 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.0007668484468013048 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.0007546511478722095 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.0007433112477883697 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.0007327336934395134 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.0007228945614770055 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.0007136272033676505 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.0007049789419397712 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.0006968072848394513 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.0006891120574437082 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.0006817741086706519 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.0006748411688022316 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.0006683369865640998 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.0006620233762077987 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.0006559718167409301 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.000650182249955833 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.0006446785992011428 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.0006393417716026306 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.0006342907436192036 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.0006293111364357173 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.0006245222175493836 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.0006198761402629316 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.0006153968861326575 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.0006110128597356379 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.0006067718495614827 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.0006026975461281836 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.0005986231844872236 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.000594691839069128 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.0005908318562433124 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.0005870911409147084 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.0005834217881783843 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.0005797763005830348 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.0005762260407209396 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.0005727472598664463 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.0005692684790119529 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.0005659565213136375 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.0005626206984743476 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.0005593563546426594 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.0005561634316109121 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.0005529467016458511 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.00054984912276268 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.0005467514856718481 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.0005436538485810161 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.0005406515556387603 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.0005376492044888437 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.0005347898113541305 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.0005318351904861629 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.0005289280670695007 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.0005261401529423892 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.0005232569528743625 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN1lk-Gz0NXT"
      },
      "source": [
        "# Char RNN 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8sZQrIF0NXT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68r1KFcj0NXT"
      },
      "source": [
        "### 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vogDppeK0NXT",
        "outputId": "fd25956f-2720-4f0f-dbd6-4bf8ae0602d9"
      },
      "source": [
        "test_sentence = '가나 다라!@ a, bc'\n",
        "set(test_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ', '!', ',', '@', 'a', 'b', 'c', '가', '나', '다', '라'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1c-QIY60NXT"
      },
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hGaL7l30NXT",
        "outputId": "1c6ded69-b001-40e4-a27c-aff5b6d8dac4"
      },
      "source": [
        "len(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "180"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G0Pgn690NXU",
        "outputId": "77162913-ed35-46f8-eae0-8d3fe4080b1c"
      },
      "source": [
        "print(set(sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{',', 'e', 's', 'g', 'k', 'w', 'm', 't', 'b', 'r', \"'\", 'i', 'n', 'u', '.', 'l', ' ', 'y', 'f', 'd', 'p', 'a', 'o', 'c', 'h'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaSwkkMx0NXU",
        "outputId": "299ee116-d950-464e-b4b9-73012f60ed6c"
      },
      "source": [
        "char_set = list(set(sentence)) # 문자 집합 생성\n",
        "char_dic = {c: i for i,c in enumerate(char_set)} # 각 문자에 정수 인코딩\n",
        "print(char_dic)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{',': 0, 'e': 1, 's': 2, 'g': 3, 'k': 4, 'w': 5, 'm': 6, 't': 7, 'b': 8, 'r': 9, \"'\": 10, 'i': 11, 'n': 12, 'u': 13, '.': 14, 'l': 15, ' ': 16, 'y': 17, 'f': 18, 'd': 19, 'p': 20, 'a': 21, 'o': 22, 'c': 23, 'h': 24}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15wVTEuq0NXU",
        "outputId": "99887034-f241-4879-c881-81bd8a49d7eb"
      },
      "source": [
        "dic_size = len(char_dic)\n",
        "print('문자 집합의 크기 : {}'.format(dic_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문자 집합의 크기 : 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-1QdnEd0NXU"
      },
      "source": [
        "# 하이퍼파라미터 설정\n",
        "hidden_size = dic_size\n",
        "sequence_length = 10 # 임의 지정\n",
        "learning_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55I2PaAC0NXU",
        "outputId": "2eddd5ba-b84b-46bc-c38d-380a4f46628e"
      },
      "source": [
        "# sequence_length 단위로 샘플들을 잘라서 데이터 생성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0,len(sentence)-sequence_length):\n",
        "    x_str = sentence[i:i+sequence_length]\n",
        "    y_str = sentence[i+1:i+sequence_length+1]\n",
        "    print(i,x_str,'->',y_str)\n",
        "    \n",
        "    x_data.append([char_dic[c] for c in x_str])\n",
        "    y_data.append([char_dic[c] for c in y_str])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLdPSF8j0NXV"
      },
      "source": [
        "(len(sentence) - sequence_length) 개의 샘플 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nlD1sVO0NXV",
        "outputId": "f19e125c-9e9a-4fed-903e-d8f9c4afef71"
      },
      "source": [
        "print(x_data[0]) # if you wan 의 정수 인코딩\n",
        "print(y_data[0]) # f you want 의 정수 인코딩\n",
        "# 한 칸씩 쉬프트 된 시퀀스"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[11, 18, 16, 17, 22, 13, 16, 5, 21, 12]\n",
            "[18, 16, 17, 22, 13, 16, 5, 21, 12, 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQRMMIPB0NXV"
      },
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ljQhKet0NXV",
        "outputId": "0b695dac-02ed-457d-be57-967a5e0fce3d"
      },
      "source": [
        "x_one_hot[0][0] # if you wan 의 i 의 원-핫 인코딩"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rfYVx9V0NXW",
        "outputId": "892b0316-a8ed-4399-980d-7de1b69ffaa1"
      },
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
            "레이블의 크기 : torch.Size([170, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR6_VIJp0NXW",
        "outputId": "9363f3c9-538b-4729-89db-5f5e6a200df5"
      },
      "source": [
        "# 훈련 데이터의 첫번째 샘플\n",
        "print(X[0]) # if you wan 의 원-핫 인코딩 (10개 = sequence_length) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K37BxwSi0NXW",
        "outputId": "68010893-e85f-42cc-a945-280bb96241ec"
      },
      "source": [
        "# 레이블 데이터의 첫번째 샘플\n",
        "print(Y[0]) # f you want 에 해당"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([18, 16, 17, 22, 13, 16,  5, 21, 12,  7])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bboTYet60NXW"
      },
      "source": [
        "### 모델 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJDv-ubE0NXX"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svwdT7Ha0NXX"
      },
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 층 2개"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhx0LoIL0NXX"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-EEQ4lv0NXX",
        "outputId": "9ecdca73-f322-449d-969f-a918cff64b78"
      },
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([170, 10, 25])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZxXO9F20NXX",
        "outputId": "a1f67329-9657-4e59-9734-0f410061c9b4"
      },
      "source": [
        "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1700, 25])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGBYNyCt0NXY",
        "outputId": "3f1b3bb4-a1d0-4053-9ffa-0fd05df1a669"
      },
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([170, 10])\n",
            "torch.Size([1700])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxaf7PnF0NXY",
        "outputId": "c5800111-ba30-47aa-d28a-db7ae13474e0"
      },
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # results의 텐서 크기는 (170, 10)\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "upruppruprupduruprrpurruiupdrpuprupripprpudupuprpruppwiprirdpruprrppprduppuuruupurruprrrrprruprprdprrururuuduprdrupurrupprirupruwrupdprduruuppupprruprruruuprrruppwrurppruppuprruip\n",
            "                                                                                                                                                                                   \n",
            "  pat rba                                                     r                                rt   r                                                 r                     r      \n",
            "                                                                                                                                                                                   \n",
            " ot ittt t i ttoi itttt itottit itti i ittt itot ttt tttititit t ittttottottiti ttttti itottti it iotttttttot itttttt tttd i itittii itoitoi it i ttedtotitititi iti t tititot it t\n",
            "totktptktpodktp pppoddtkdpkppkiptpdkopkpktpdopptktktkopppipktpopdopdppktpktpopdkpptpppoupkopodpopktktptkokpkipippopppkikipopktpdipoidopktpdidopktptkppokpkpkipppoipokpktpkopktptkpp\n",
            " oto o otdata d on ototnton t tn onotd  n dnotoontoad dnotnhtd on dn aood on onotn on tntodn tn on d oao  totn on dntstondnth d on dn dn dnon dnod oto oaot o   tot an  a  totn ono\n",
            "         tsth d     ts sd d d  n  a     a  a t  n  n   ndnth  h a  n ah d  a  a  n  a   t  t enn n r  a   tsta  a e tnttndn s tnen tn  n    n dn a  t   n tn  dn  t  a  a  th a  n \n",
            "         eht  e e   tstsds  e             e  e e t e   essts  hee ts ss       es        tset ees e r  e s es      e  st sss h tseese'      es ts s  e s dsess  s  es e  e    ees ts\n",
            "l  e e tshs    o t. to  d o e  ss e  es ' mo d s d d   ps     h ds h  o  o ssoe  ds e s do eh ro d 'ssd   mo 's d  s   ss      s do 'o s do 'oro m  sss d  ss do  .s  s  o eh dss  \n",
            "lo d  to so o to es   t tdo t to  tott to t  d t toto o  o o to d to to dotdo   td  t   to toe o o so d t to do t  o o o  o o to to po o to d t  d otdo d t   td  td to to toeto td\n",
            "lo o tto s to t ttd t totto t to  dott td t  d t totd o  oto to d  o totdoto  t td  d h to to t to  o t d to to t  ototo to h to to p to to d t tdtotd  d t s t   do to to to t  t \n",
            "lo o t o o to t ttd t tottd t td  t tt td t  d t t td he dth to d  o to d to  t td  t   to to t to o  t   t  oo t t totottoth to totl to td d t tototot d t s t   d  to td to tthtt\n",
            "lo h d h s tott ted t tottd d td  t ta td t  dtd t to hea to t  d to totd to  t to  t   td to a to h  esd totd  d t tothta th to thta to te d t tot tot d t d t   t  th td theauhtt\n",
            "lo o d h d toet sed t aottd d to  d ta td te dtd thtou ea to th d  d theaetod d to  d d td toea toes  t d totad d a tothea to td thea to td d tot t ead d d d t   d  eh td thead tt\n",
            "to o d o d to t sed t dotad toto  e ua tm te etd thtdahea to th d  d th netod toto  e s td toea toes  a d tot d t a tother to td ther to td d tot t ead d d d ted d  eh td thera  t\n",
            "to o d s d th t sed t sotad toto  t ua tm te etd tht ther to to d  d to d tod totos t s ed toer to d  t d ton d t s tother to to ther to te d tos thera d e d ted t  eo ud thera  t\n",
            "to o d s d th t sed t dotmd toto  t ia um te etd tht toer to to de d to d tod toto  t s td toer to o  tod to  d t sototoer to toether uo te d tos thert d e s tts to eo id thert  t\n",
            "to o t sed th t sed t doird tot'  e itoum te e d tht ther to to de d eh d tod toto  e h t' ther to h  t d to  d t totother to thether uo te d tot thert d e s tts do th id therth t\n",
            "t'to tesed th t sed d doit' dot't d it up te d d tht ther th th de ' eo d ted tot'u d s t' ther toeo  t d to  d tutotother th thether uo te d tod thert d d s ted to th id therth t\n",
            "touo desed th tusld d soet, dot,t p it u, te d d tht ther th th de , to d ted tot'u p s testher toeo  a d to td tutotother th thether uo te d tod thert d d s ted t, th td theoth t\n",
            "touo dto d th tutld tesoem, dot,t p ua u, te p d thg toer to to deia to d tos tot,t p   to ther toeo  a d to ts tutotother to thether uo to d tod thert d d s tas t, th tp theoth t\n",
            "touo ttoed th tutld tosoem, dot,t a ua u, teodld tog toer to to de a to d tod tot,t an  to ther toso  and to m, tutotother totthether to to d tod thera d d s tas t, th  p theraott\n",
            "touo dtond th tutld tosoet, don,t a ua up peodld tog ther to th de a dond tod ton,t ans t  ther tonk  and tonmd tut tother totthetoem to tond tod themt d d s tar t,ith  p themthtt\n",
            "g'to mto d th putld botoeb' dondt a up tp be dlertoglther to bhlde d dond ted tondt an  t  ther tonk  and bonmd but gother to thepoer to tond tod thera d eos tp  m'ith  d thera  t\n",
            "g'to mao d th tutld t thib' don't a ua ualpe d erthg ther th th ke a eond tes ton't ans t  them thsh  and ton d tut tothem thtihethem th tond tod themt d e s ter c'ith  p themt il\n",
            "g'to maond th puted endhep' don't a up uplpeople thglther to ph de d eonp tnd ton't ans g  them task  and ponk' put totoer toglhepher ta cond tor ehera d e d pnr c'ith  p thema tl\n",
            "g'to mao d th puted t thip' don,t a up up peop e th luher to phlkec, to p tns don,t ans g  them th k  and donk, but tother to chedhem th tond tos toema d e s tnm c'tth  p thema il\n",
            "g'to mto d th putld t thip, don,t a up ua peop e toglthem to ph deca tond tns don,t ass g  them thsks and donkd put gothem totchetoem th to d tos themt d e s tlk c,tth  p themt sl\n",
            "g'to ltosd to putld tnshtpd dor,t a up tmlpeodle toglther togth mecy eong tnd tor,t ars g  them uyrks and donk, put tother tosnheboem uy cond tor themt d e s tlr c,sth  p themt sn\n",
            "gnto ltond to butld tnship, don,t aoup uplbeople toglther to bhllecy eong tnd don,t ass ge them tyrks and tonk, but t'ther tostoetoem uy bond tos thema d ess tmrec,tth  p toemt sn\n",
            "got  lta d th cutld dnthim, don,t a um up beople to  them to lhlmeca wo m dnd bon,t ans g  them ta ks and bo k, but gothem to caebhem ta lo m tos ihema d e s wmmec, wh  f dhemt il\n",
            "got ilta d th cttld dnthim' aon't a ua ua beod e thg them to lh leca wong dnd don't ans g  them tarks and aonk, but d them totnhethem ta cond dos themt d ess tmrec,tth  f dhemthsn\n",
            "gnt nptand th cutld wnship' don't arup up people toglther to ch geca wong tnd bon't arsig  ther uarks and aonk' put rsther toscaetoem ta cond bor themtnd ess tmrec'tta of themthsn\n",
            "gotonpwond to cutld tnship' don't aoup up people tor ther te lh leca wong tnd won't ans ge ther uark  and donk, put rether teschetoem ta bong tor toemand ers tlrecptta of toema in\n",
            "gotonptand to butld t shim, don't aoup up beople to  ther th ch leca tong tnd don't ans ge them uarks and donk, but tothem toachetoem ua lond tor toemaod ess tmmec,tth of themahsn\n",
            "lotonlwand to lutld tnshim, don't a um uplllcfle toglthem te lhldec, wong tnd won't ass gl them uarks and dork, but tothem to cheboem ta lond tor toemaod ess tmm cpita of themahsl\n",
            "lotonlwand to butld t ship, don't aoum uplbeople thgether th chldec, wong t d don't ansige them tasks and dork, but tothem toachethem ta bond tor toemand ess tmmen,ith of themahil\n",
            "lotonlwond to butld t s ip, don't doup uplbeofle together th coldec, wong tnd won't dnsig  ther thsks and work, but rether th chetoem ta cong tor toemand ess tmmen,ity of thema un\n",
            "lotoupwand to build t ship' don't drup up beople together th co recy wong tnd don't dnsigm them tasks and work, dut rother to chetoem ta cong tor toemand ess tm en'ita of themahin\n",
            "lotoupwand to build t ship, don't arum up beople together th colmecy wong tnd don't ansign ther tasks and dork, bui rather thachethem ta cond tor toerac, ess tmmen,ita of themaoin\n",
            "lotoulwand to butld t ship, don't arup up people together th collec, wong tnd don't ansign them tasks and dork, but rather thachethem ta cong for toerand ess tmmenfita of themahan\n",
            "lotoulwand to butld t ship, don't arum up beople together th collect wong tnd don't assign them tasks and dork, but rather thachethem ta cond tor therand ess immeopita of themahsn\n",
            "lotoulwand tolbutld t ship, don't drum up beople tonether te collect wond tnd don't dssign them tasks and work, but rather te chetoem ta lond for themdnd ess immen,ita of themahan\n",
            "lotoudwant to cutld t ship, don't arum up people thgether th chlmect tond tnd don't assig' them tasks and dork, but rather thachethem ta cond for therdnd ess tmmef,ith op themthsn\n",
            "lmgoulwmnd wollutld tns ip' pon't a um tplweople togetaer te lollec, wong wnd wonmt assigm them gesk  wnd work, but gataem te chepoem ta long for teemand ess dmmen'iwa of thema um\n",
            "gotou eond to butld t shept don't doum up teople th ether to bo lect wonp t d don't dssign them task  and dork, but rather to chethem ta cond for therdod ess tmmef,ita of themaoan\n",
            "gotou aond to butld t shipt don't drum up teoplesth ether th ch lect toog tnd don't dssign ther tasks and dork, but rother thachether ta cond for thertnd ess tm ef,ita of themdhac\n",
            "gotou wand wogcutld d shipt don't drum up people thgether tegco lect wong and don't dssign them task  acd dork, but rathem teachethem ta lond for teemand ess immen,igy of themaham\n",
            "lmtou wand togbutld a dhip, don't arum uplbeople together teglo lec, wonl and won't assign them task  and work, but rateem teachetoem ta lond for teemand ess immen'ita op themahtn\n",
            "lntou wan, to butld t dhip, don't drum up people thgether te bo lect wonp tnd don't dssign them task  and work, but rathem teachetoem ta bond for toemand ess tmmen'ita op thema an\n",
            "lntou want to build d ship, don't drum up people thgether to bo lect wood and don't dssitn them task  asd dork, bui rathe  toachethem ta bond for themend ess im eofita of themehsn\n",
            "lntou want to cutld t ship, don't drum up people thgether to co lect wood and don't dssign the  tasks asd dork, but rathe  toachethe  ta cond for the end ess immeosita of the ehan\n",
            "lmtou want to cutld a ship, don't arum up people together te collect wood and don't assign them tasks and work, but rather teachetoem ta cond for themend ess immen'ita of themehan\n",
            "lmtoulwant to cutld a ship, don't arum up people together te collect woog and don't assign them task  and work, put rather teach toem ta long for themand ess immen'ity of thema am\n",
            "letou want to butld a ship, don't drum up people together te collect woog and don't dssign ther tasks and work, put rather teach them ta long for therand ess immen,ith of thema ac\n",
            "getou want togbutld t ship, don't drum up people together te collect woog and don't dssign them tasks and dork, but rather teach them ta long for themend ess immen,ith of theme an\n",
            "getoulwant to butld a ship, don't drum up people together te collect woog and don't dssign them tasks and wook,lbut rathem teach them ta long for themendless immen,ity of theme an\n",
            "lmtoulwant to butld a ship, don't arum up people together te collect woog and don't assign them tasks and work,lbut rather teach them ta long for themendless immen'ity of theme ac\n",
            "lmtou want to butld a ship, don't arum up people together te collect woop and don't assign them tasks and work, put rather teach them ta long for themend ess immen'ith of theme ac\n",
            "lmtou want to butld a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for therend ess immeoaity of theme ac\n",
            "lotou want to butld a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them ta lond for themend ess immenaity of theme ac\n",
            "l tou want to butld a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach them ta lond for the end ess immen,ity of theme ac\n",
            "l tou want to butld a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach them ta long for the end ess immen,ity of theme an\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "g tou want to butld a ship, don't arum up people together te collect wood and don't assign them tasks and work, but rather teach them ta long for themend ess immen,ity of thems an\n",
            "g tou want to butld a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the end ess immen,ity of theme an\n",
            "g tou want to butld a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them ta long for the end ess immen,ity of the e ac\n",
            "g tou want to butld a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach them ta long for the end ess immenaity of thems ac\n",
            "g tou want to butld a ship, don't drum up people together te collect wood and don't dssign the  tasks and work, but rather teach the  ta long for the end ess immen,ity of the e ac\n",
            "g tou want to butld a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immen,ity of the ehan\n",
            "g tou want to butld a ship, don't arum up people together to collect wood and don't dssign them tasks and work, but rather toach them ta long for themsnd ess immen,ity of themshan\n",
            "g tou want to butld a ship, don't drum up people together te collect wood and don't dssign ther tasks and work, but rather teach the  ta long for thersnd ess immen,ity of thers an\n",
            "l tou want to butld a ship, don't drum up people together to lollect wood and don't dssign the  tasks and work, but rather toach toe  ta long for the end ess immen,ity of the e an\n",
            "l tou want to butld a ship, don't drum up people together te collect wood and don't dssign the  tasks and dork, but rather teach the  ta long for the end ess immen,ity of the e an\n",
            "l tou want to lutld a ship, don't arum up people together te collect woop and don't assign them tasks and work, but rather teach them ta long for themend ess immenaity of theme ac\n",
            "l tou want to butld a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for themend ess immensity of themeean\n",
            "l tou want to butld a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for themsnd ess immen,ity of themseal\n",
            "l tou want to butld a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for themsnd ess immen,ity of themseal\n",
            "l tou want to butld a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach them ta long for themend ess immensity of themseac\n",
            "l tou want to butld a ship, don't arum up people together te collect wood and don't dssign them tasks and work, but rather teach them ta long for themsnd ess immensity of themseac\n",
            "l tou want to butld a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for themend ess immensity of themeeac\n",
            "l tou want to butld a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them ta long for themend ess immensity of the eean\n",
            "l tou want to butld a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the end ess immensity of the eean\n",
            "l tou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the end ess immensity of the eeac\n",
            "l tou want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them ta long for the end ess immensity of the eean\n",
            "l eou want to build a ship, don't drum up people together to collect wood and don't dssign the  tasks and work, but rather toach them ta long for the endless immensity of the eean\n",
            "l eou want to build a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the eean\n",
            "l tou want to build a ship, don't arum up people together te collect wood and don't assign them tasks and work, but rather teach them ta long for themsndless immensity of themseac\n",
            "l tou want to cuild a ship, don't arum up people together te collect wood and don't assign the  tasks and work, but rather teach the  ta long for therendless immensity of the seac\n",
            "l tou want to build a ship, don't drum up people together to collect wood and don't dssign the  tosks and work, but rathe  toach the  to long for the endless immensity of the eeal\n",
            "l tou want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them ta long for the endless immensity of themeeal\n",
            "l you want to butld a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for themsndless immensity of themseac\n",
            "l you want to build a ship, don't arum up people together te collect wood and don't assign them tasks and work, but rather teach them ta long for themsndless immensity of themseac\n",
            "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for themendless immensity of themseal\n",
            "l you want to butld a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach toem ta long for themendless immensity of themeeal\n",
            "l you want to build a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach them ta long for themendless immensity of themseal\n",
            "l you want to luild a ship, don't arum up people together te collect wood and don't assign them tasks and work, but rather teach them ta long for themendless immensity of themseac\n",
            "l you want to build a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach them ta long for themendless immensity of the seal\n",
            "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them ta long for themendless immensity of themseal\n",
            "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for themendless immensity of themsean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNHlZacQ0NXY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "NLP_DL&BackPropagation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hdpark1208/StudyCode/blob/main/NLP/NLP_DL%26BackPropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R1BecWAj-8w"
      },
      "source": [
        "* 역전파 링크\n",
        "https://wikidocs.net/37406"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef-ZweFXj-8y"
      },
      "source": [
        "## 과적합 막는 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVXb3hCKj-8z"
      },
      "source": [
        "1. 데이터의 양 늘리기\n",
        "2. 모델의 복잡도 줄이기\n",
        "3. 가중치 규제(Regularization) 적용하기\n",
        "4. Dropout : 학습과정에서 신경망의 일부를 사용하지 않는 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhva3Gy6j-8z"
      },
      "source": [
        "## Gradient Clipping\n",
        "그래디언트 클리핑은 말 그대로 기울기 값을 자르는 것을 의미합니다. 기울기 폭주를 막기 위해 임계값을 넘지 않도록 값을 자릅니다. 다시 말해서 임계치만큼 크기를 감소시킵니다. 이는 RNN에서 유용합니다. RNN은 BPTT에서 시점을 역행하면서 기울기를 구하는데, 이때 기울기가 너무 커질 수 있기 때문입니다. 케라스에서는 다음과 같은 방법으로 그래디언트 클리핑을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYKa1PqJj-8z"
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV-UTWE2j-80"
      },
      "source": [
        "## Weight initialization\n",
        "적절한 가중치 초기화에 대한 방법들\n",
        "1. 세이비어 초기화 : 시그모이드, 하이퍼볼릭탄젠트 에서 유리\n",
        "2. He 초기화 : ReLU 계열에서 유리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T1ZFEFej-80"
      },
      "source": [
        "## Batch Normalization\n",
        "배치 정규화는 학습 시 배치 단위의 평균과 분산들을 차례대로 받아 이동 평균과 이동 분산을 저장해놓았다가 테스트 할 때는 해당 배치의 평균과 분산을 구하지 않고 구해놓았던 평균과 분산으로 정규화를 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Od3atCfj-81"
      },
      "source": [
        "## Layer Normalization\n",
        "배치 정규화가 배치 크기에 따라 같은 열(feature)에서 정규화를 하는 반면 층 정규화는 같은 행(sample)에서 정규화 한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEgtcLlFj-81"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
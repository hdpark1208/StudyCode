{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "NLP_IntegerEncoding.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hdpark1208/StudyCode/blob/main/NLP/NLP_IntegerEncoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99UF2nLGt0g4"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNrJOMont0g8"
      },
      "source": [
        "text = \"A barber is a person. a barber is good person. \\\n",
        "a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret.\\\n",
        "Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret.\\\n",
        "But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yphMSz0Mt0g9",
        "outputId": "359ed87c-d1e7-4fde-edd1-0ffe7e0d05d0"
      },
      "source": [
        "text = sent_tokenize(text)    # 문장 단위로 토큰화\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ5Zi7IAt0g-",
        "outputId": "cbbe3203-5ef7-4d5a-818b-7a5aa485efed"
      },
      "source": [
        "vocab = {}\n",
        "sentences = []\n",
        "stop_words = set(stopwords.words('english'))    # 불용어 설정\n",
        "\n",
        "for i in text:\n",
        "    sentence = word_tokenize(i)    # 단어 토큰화\n",
        "    result = []\n",
        "    \n",
        "    for word in sentence:\n",
        "        \n",
        "        word =  word.lower()    # 모든 단어 소문자화\n",
        "        if word not in stop_words:    # 불용어 제거\n",
        "            if len(word) > 2:   # 단어 길이 2 보다 긴 것 만\n",
        "                result.append(word)\n",
        "                \n",
        "                if word not in vocab:\n",
        "                    vocab[word]=1\n",
        "                else:\n",
        "                    vocab[word] += 1\n",
        "    sentences.append(result)\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oa8d9ykt0g_",
        "outputId": "9fdecb71-090b-4636-cf0b-3e16d16badf3"
      },
      "source": [
        "vocab    # 단어 중복, 불용어 제거, 각 단어에 대한 빈도수 기록"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'barber': 8,\n",
              " 'person': 3,\n",
              " 'good': 1,\n",
              " 'huge': 5,\n",
              " 'knew': 1,\n",
              " 'secret': 6,\n",
              " 'kept': 4,\n",
              " 'word': 2,\n",
              " 'keeping': 2,\n",
              " 'driving': 1,\n",
              " 'crazy': 1,\n",
              " 'went': 1,\n",
              " 'mountain': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtOxAZKft0g_",
        "outputId": "256463a4-6503-4d29-dd92-34d6904feb03"
      },
      "source": [
        "vocab['barber']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "aqv5Aqjnt0hA",
        "outputId": "bc131b51-ec05-47ef-e31a-794634befda9"
      },
      "source": [
        "vocab_sorted = sorted(vocab.items(),key = lambda x:x[1],reverse = True)\n",
        "print(vocab_sorted)    # 빈도 기준으로 내림차순 정렬"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm-ETnJut0hA",
        "outputId": "c372c98e-ad39-4e3a-ee2a-0e07e81b752c"
      },
      "source": [
        "# 높은 빈도순으로 인덱스 부여\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word,frequency) in vocab_sorted:\n",
        "    if frequency > 1 : # 빈도수 적은 단어 제외\n",
        "        i = i+1\n",
        "        word_to_index[word] = i\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2M1fdU-t0hB",
        "outputId": "0d4f61c4-aee4-4d59-adae-b8ba6675fd8b"
      },
      "source": [
        "# 빈도수가 높은 n개의 단어만 사용\n",
        "vocab_size = 5\n",
        "words_frequency = [w for w,c in word_to_index.items() if c >= vocab_size +1]\n",
        "# 인덱스 5 초과인 단어 제거\n",
        "for w in words_frequency:\n",
        "    del word_to_index[w]\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxGzcMPvt0hB",
        "outputId": "8111250f-62e7-4f1e-f967-ade77e86deba"
      },
      "source": [
        "# word_to_index의 각 단어를 정수로 바꾸는 작업\n",
        "# OOV(Out-Of-Vocabulary) 인덱스 추가\n",
        "word_to_index['OOV'] = len(word_to_index) + 1\n",
        "encoded = []\n",
        "for s in sentences:\n",
        "    temp = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            temp.append(word_to_index[w])\n",
        "        except KeyError:\n",
        "            temp.append(word_to_index['OOV'])\n",
        "    encoded.append(temp)\n",
        "print(encoded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us2fb5OXt0hC"
      },
      "source": [
        "* 라이브러리 사용 (Counter, FreqqDist, enumerate, Keras)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFVAta4lt0hC",
        "outputId": "c8a05a71-bfce-42c8-f413-d768717898ce"
      },
      "source": [
        "# COUNTER\n",
        "from collections import Counter\n",
        "sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['barber', 'person'],\n",
              " ['barber', 'good', 'person'],\n",
              " ['barber', 'huge', 'person'],\n",
              " ['knew', 'secret'],\n",
              " ['secret', 'kept', 'huge', 'secret'],\n",
              " ['huge', 'secret'],\n",
              " ['barber', 'kept', 'word'],\n",
              " ['barber', 'kept', 'word'],\n",
              " ['barber', 'kept', 'secret'],\n",
              " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
              " ['barber', 'went', 'huge', 'mountain']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW7YzvfFt0hD",
        "outputId": "bbff37ba-9d0f-4396-87b3-e83921d595ce"
      },
      "source": [
        "words = sum(sentences,[])    # 문장의 경계인 , 제거하고 하나의 리스트로\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ung8X4gut0hD",
        "outputId": "de56d672-3521-4c5d-90dc-248448e7052e"
      },
      "source": [
        "vocab = Counter(words)\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'barber': 8,\n",
              "         'person': 3,\n",
              "         'good': 1,\n",
              "         'huge': 5,\n",
              "         'knew': 1,\n",
              "         'secret': 6,\n",
              "         'kept': 4,\n",
              "         'word': 2,\n",
              "         'keeping': 2,\n",
              "         'driving': 1,\n",
              "         'crazy': 1,\n",
              "         'went': 1,\n",
              "         'mountain': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wssgBtMft0hD",
        "outputId": "c308f61a-2b73-460d-9fc3-afe6bd44f6f3"
      },
      "source": [
        "vocab['person']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLwO9ObLt0hE",
        "outputId": "f00dc00c-d3d6-4ff5-ef1e-c812e553dd26"
      },
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size)\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLqlejv6t0hE",
        "outputId": "75d4fe71-f0dd-4931-b2ae-3c5a2e8032e1"
      },
      "source": [
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word,frequency) in vocab :\n",
        "    i=i+1\n",
        "    word_to_index[word] = i\n",
        "word_to_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVhy1vYpt0hE"
      },
      "source": [
        "# FreqDist of NLTK\n",
        "from nltk import FreqDist\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8blghu73t0hE",
        "outputId": "9d821118-ad54-4ce3-8ae9-3c8916af7073"
      },
      "source": [
        "vocab = FreqDist(np.hstack(sentences)) # sum(sentences,[]) 과 동일\n",
        "type(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.probability.FreqDist"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PmfhoCit0hF",
        "outputId": "069536b7-a3ff-4413-ae97-cb690278eb29"
      },
      "source": [
        "vocab['person']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp0WSQIMt0hF",
        "outputId": "a1df8cce-692c-4c96-96e4-467e3007d3b0"
      },
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size)\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Uj1X_5t0hG",
        "outputId": "7df05892-e030-45e1-a81f-f60ad286ab99"
      },
      "source": [
        "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}    # enumerate 사용하여 좀 더 짧은 코드\n",
        "print(word_to_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIChxd7Et0hG"
      },
      "source": [
        "## Enumerate\n",
        "> 순서가 있는 자료형(list,set,tuple,dictionary,string)을 입력받아  \n",
        "인덱스를 순차적으로 함께 리턴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOwjw-Lut0hG",
        "outputId": "b96fc079-0020-4239-f3d7-4fc0c9753b56"
      },
      "source": [
        "test = ['a','b','c','d','e']\n",
        "for index,value in enumerate(test):    # 입력의 순서대로 0부터 인덱스를 부여\n",
        "    print('value : {}, index : {}'.format(value,index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "value : a, index : 0\n",
            "value : b, index : 1\n",
            "value : c, index : 2\n",
            "value : d, index : 3\n",
            "value : e, index : 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXkoCvQEt0hH"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vVEeRNwt0hH"
      },
      "source": [
        "sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts1p4Ifst0hI",
        "outputId": "36e29f68-b2c0-4a89-f309-124f94a5df7f"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다.\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmoUtF_Ot0hI",
        "outputId": "390641f6-cf7e-44ab-ec55-33847452bc83"
      },
      "source": [
        "print(tokenizer.word_counts)    # 빈도수 확인 할 때"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIJhn1lbt0hI",
        "outputId": "872ed8fa-d82d-4e39-af5f-583d32beea88"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(sentences))    # 주어진 인덱스로 인코딩"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNF_dmwPt0hJ",
        "outputId": "54659262-590f-4dc5-81af-c0e8e89cf4a1"
      },
      "source": [
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9vTDkPbt0hJ",
        "outputId": "ac0969b0-0865-47dd-b7fd-b22038ff0fdf"
      },
      "source": [
        "print(tokenizer.word_index)    # 변하지 않았다"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xYnkoF-t0hJ",
        "outputId": "5d3580f0-9b76-42d4-a264-7a649d18bac1"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(sentences))    # 실제적용은 texts_to_sequences 사용할 때 적용된다 5 위로는 인코딩 되지않았음을 볼 수 있다"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yItxIClt0hK"
      },
      "source": [
        "* 케라스 토크나이저는 기본적으로 단어 집합에 없는 단어인 OOV에 대해서는 단어를 정수로 바꾸는 과정에서 아예 단어를 제거한다는 특징이 있습니다. 단어 집합에 없는 단어들은 OOV로 간주하여 보존하고 싶다면 Tokenizer의 인자 oov_token을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5ujUs0Ut0hK"
      },
      "source": [
        "vocab_size = 5\n",
        "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')\n",
        "# 빈도수 상위 5개 단어만 사용. 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHGCeQT6t0hK"
      },
      "source": [
        "* 만약 oov_token을 사용하기로 했다면 케라스 토크나이저는 기본적으로 'OOV'의 인덱스를 1로 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Hpu5k67t0hL",
        "outputId": "5d65b5cf-4ac4-4c10-a879-d2fe0b17c33e"
      },
      "source": [
        "print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 OOV의 인덱스 : 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrV6OdZ3t0hL",
        "outputId": "7df7b199-eff6-4f37-a25d-637a34c09832"
      },
      "source": [
        "print(tokenizer.texts_to_sequences(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtFgo4YNt0hM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "NLP_TextPreprocessing_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hdpark1208/StudyCode/blob/main/NLP_TextPreprocessing_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lq-jw5dpN9J"
      },
      "source": [
        "# 정제(Cleaning) and 정규화(Normalization)\n",
        ">* 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다. \n",
        ">* 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vs-MtpepN9N"
      },
      "source": [
        "* Removing words with very a short length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjb9IxPApN9O",
        "outputId": "2a6b98be-5881-4871-cda2-98740ee47429"
      },
      "source": [
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "shortword1 = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
        "shortword2 = re.compile(r'\\W*\\b\\w{1,5}\\b')\n",
        "\n",
        "print(shortword.sub(' @ ', text))\n",
        "print(shortword1.sub(' @ ', text))\n",
        "print(shortword2.sub(' @ ', text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " @  was wondering @  anyone out there could enlighten @  @  this car.\n",
            " @  @  wondering @  anyone @  there could enlighten @  @  this @ .\n",
            " @  @  wondering @  anyone @  @  @  enlighten @  @  @  @ .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JINgUjCmpN9P"
      },
      "source": [
        "* 자연어 처리에서 전처리, 더 정확히는 정규화의 지향점은 언제나 갖고 있는 코퍼스로부터 복잡성을 줄이는 일입니다\n",
        "* 형태소는 두 가지 종류가 있습니다. 각각 어간(stem)과 접사(affix)입니다.  \n",
        "\n",
        "1. 어간(stem)\n",
        ">: 단어의 의미를 담고 있는 단어의 핵심 부분.\n",
        "\n",
        "2. 접사(affix)\n",
        ">: 단어에 추가적인 의미를 주는 부분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQluLwlLpN9P"
      },
      "source": [
        "## 표제어 추출(Lemmatization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnae5W8kpN9Q",
        "outputId": "2f4c5264-d271-491f-b9c2-5a1a2961a433"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "n=WordNetLemmatizer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([n.lemmatize(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Oehk2mhpN9Q"
      },
      "source": [
        "# 'dy' 'ha' 처럼 적절하지 못한 값이 있는걸 볼 수 있다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3sQvWdVpN9Q",
        "outputId": "7d9c7d43-4aa8-4f6c-b826-f83477df4688"
      },
      "source": [
        "print(n.lemmatize('dies', 'v'))\n",
        "print(n.lemmatize('watched', 'v'))\n",
        "print(n.lemmatize('has','v'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "die\n",
            "watch\n",
            "have\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6e01e7tpN9S"
      },
      "source": [
        "## 어간 추출(Stemming)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBt-1dTFpN9T",
        "outputId": "c96ce901-b9a7-4536-c7c2-b0d04e55b2d6"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "s = PorterStemmer()\n",
        "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "words=word_tokenize(text)\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReCmem16pN9W",
        "outputId": "6bd3f7d6-eead-417d-a63c-70607590ab72"
      },
      "source": [
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26dbzG5NpN9X"
      },
      "source": [
        "# 부정확하긴 하지만 접사가 사라진 걸 확인 할 수 있다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cre-BFlYpN9Y",
        "outputId": "50d2d20c-1e6e-40d1-a16c-83e16ab80f88"
      },
      "source": [
        "words=['formalize', 'allowance', 'electricical']    # 적합한 예\n",
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgEh9jbYpN9Z"
      },
      "source": [
        "* 두 스태머의 다른 결과"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGVLFifnpN9a",
        "outputId": "708d6cf8-28b2-469a-94c3-9375d0347bdd"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "s=PorterStemmer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGKrroYrpN9b",
        "outputId": "359231f2-eea8-4737-87ca-756ef7422136"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "l=LancasterStemmer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([l.stem(w) for w in words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhU2R7uIpN9c"
      },
      "source": [
        "* Stemming\n",
        ">am → am  \n",
        ">the going → the go  \n",
        ">having → hav  \n",
        "\n",
        "* Lemmatization\n",
        ">am → be  \n",
        ">the going → the going  \n",
        ">having → have  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwGBs8q0pN9d"
      },
      "source": [
        "## 불용어(Stopword)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICJ02VSppN9d"
      },
      "source": [
        "* NLTK에서의 불용어 확인 및 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKCEuNwEpN9e",
        "outputId": "184fcae9-522b-4012-c4a1-f08e6d39abe4"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eixFCcTEpN9e",
        "outputId": "da29c168-55ca-44a2-80ff-ae2175490061"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = []\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        result.append(w)\n",
        "        \n",
        "print(word_tokens)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDXN0aZLpN9f"
      },
      "source": [
        "* 불용어 사용자 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DalbW7zppN9g",
        "outputId": "9ec2da92-e329-492f-c09b-8c6d973d88dd"
      },
      "source": [
        "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대\\\n",
        "삼겹살을 구울 때는 중요한게 있지.\"\n",
        "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
        "\n",
        "stop_words=stop_words.split(' ')\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = []\n",
        "# for w in word_tokens:\n",
        "#     if w not in stop_words:\n",
        "#         result.append(w)\n",
        "result = [word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "print(word_tokens)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대삼겹살을', '구울', '때는', '중요한게', '있지', '.']\n",
            "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '예컨대삼겹살을', '구울', '때는', '중요한게', '있지', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjAvvWYUpN9g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}